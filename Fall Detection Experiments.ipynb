{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research experiments for the purposes of the master's thesis titled \"Artificial intelligence methods in fall detection based on accelerometer data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of falls within other activities - fall data vs ADL data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_data = scipy.io.loadmat('./UniMiB-SHAR/data/fall_data.mat')['fall_data']\n",
    "adl_data  = scipy.io.loadmat('./UniMiB-SHAR/data/adl_data.mat')['adl_data']\n",
    "fall_labels = np.zeros(fall_data.shape[0])\n",
    "adl_labels = np.ones(adl_data.shape[0])\n",
    "\n",
    "data = np.concatenate( (fall_data, adl_data) )\n",
    "labels = np.concatenate( (fall_labels, adl_labels) )\n",
    "\n",
    "x = data.reshape(data.shape[0], data.shape[1], 1)\n",
    "y = labels\n",
    "\n",
    "classes = [0, 1]\n",
    "NUMBER_OF_CLASSES = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idxs = {cl: np.where(y == cl)[0] for cl in classes}\n",
    "print(\"Number of samples in each class:\")\n",
    "print([len(class_idxs[i]) for i in classes])\n",
    "numer_of_test_samples = int(len(x)*0.2)\n",
    "print(\"Number of test samples: \", numer_of_test_samples)\n",
    "print(\"Number of samples in each class in test set:\")\n",
    "print([numer_of_test_samples//2 for i in classes])\n",
    "print(\"Number of samples in each class in train set:\")\n",
    "print([len(class_idxs[i]) - numer_of_test_samples//2 for i in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indexes of each group\n",
    "cl_indexes = {}\n",
    "cl_indexes = {cl:np.where(y==cl)[0] for cl in classes}\n",
    "\n",
    "# take the the samples to test set (equal number for each class)\n",
    "number_of_test_samples = int(len(x)*0.2)\n",
    "number_of_test_samples_per_class = number_of_test_samples // NUMBER_OF_CLASSES\n",
    "\n",
    "val_idxs = [item for cl, items in cl_indexes.items() for item in items[:number_of_test_samples_per_class]]\n",
    "train_idxs = [item for cl, items in cl_indexes.items() for item in items[number_of_test_samples_per_class:]]\n",
    "\n",
    "x_val = x[val_idxs]\n",
    "y_val = y[val_idxs]\n",
    "\n",
    "x_train = x[train_idxs]\n",
    "y_train = y[train_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on CNN-infused model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "seq_len = x.shape[1]\n",
    "n_features = 1\n",
    "epochs = 20\n",
    "learning_rate = 0.0003\n",
    "batch_size = 16\n",
    "\n",
    "regularizers = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "gru1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, return_sequences=True, **regularizers))(in1)\n",
    "gru2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers))(gru1)\n",
    "rsh1 = tf.keras.layers.Reshape( (hidden_dim*2, 1) )(gru2)\n",
    "cov1 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(rsh1))\n",
    "max1 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov1)\n",
    "cov2 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 5, activation='relu', **regularizers)(max1))\n",
    "max2 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov2)\n",
    "cov3 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(max2))\n",
    "max3 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov3)\n",
    "rsh2 = tf.keras.layers.Reshape( (seq_len, 1010) )(max3)\n",
    "tdd1 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1,**regularizers))(rsh2))\n",
    "f1   = tf.keras.layers.Flatten()(tdd1)\n",
    "d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(f1)\n",
    "\n",
    "classifier3 = tf.keras.Model(\n",
    "    inputs=[in1], \n",
    "    outputs=[d1]\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "classifier3.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history3 = classifier3.fit(\n",
    "    np.array(x_train), np.array(y_train),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=x.shape[0] // batch_size,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00003)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history3.history['val_loss'], label='val_loss')\n",
    "plt.plot(history3.history['loss'], label='loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history3.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history3.history['accuracy'], label='accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-infused model with an additional dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "seq_len = x.shape[1]\n",
    "n_features = 1\n",
    "epochs = 20\n",
    "learning_rate = 0.0003\n",
    "batch_size = 16\n",
    "\n",
    "regularizers = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "gru1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, return_sequences=True, **regularizers))(in1)\n",
    "gru2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers))(gru1)\n",
    "rsh1 = tf.keras.layers.Reshape( (hidden_dim*2, 1) )(gru2)\n",
    "cov1 = tf.keras.layers.Dropout(0.3)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(rsh1))\n",
    "max1 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov1)\n",
    "cov2 = tf.keras.layers.Dropout(0.2)(tf.keras.layers.Conv1D(seq_len, 5, activation='relu', **regularizers)(max1))\n",
    "max2 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov2)\n",
    "cov3 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(max2))\n",
    "max3 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov3)\n",
    "rsh2 = tf.keras.layers.Reshape( (seq_len, 1010) )(max3)\n",
    "tdd1 = tf.keras.layers.Dropout(0.3)(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1,**regularizers))(rsh2))\n",
    "den1 = tf.keras.layers.Dense(32, **regularizers)(tdd1) # added dense\n",
    "f1   = tf.keras.layers.Flatten()(den1)\n",
    "d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(f1)\n",
    "\n",
    "classifier4 = tf.keras.Model(\n",
    "    inputs=[in1], \n",
    "    outputs=[d1]\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "classifier4.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history4 = classifier4.fit(\n",
    "    np.array(x_train), np.array(y_train),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=x.shape[0] // batch_size,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-8)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history4.history['val_loss'], label='val_loss')\n",
    "plt.plot(history4.history['loss'], label='loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history4.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history4.history['accuracy'], label='accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-infused hyperparameter optimization - no dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "seq_len = x.shape[1]\n",
    "n_features = 1\n",
    "epochs = 20\n",
    "learning_rate = 0.0003\n",
    "batch_size = 16\n",
    "\n",
    "regularizers = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "gru1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, return_sequences=True, **regularizers))(in1)\n",
    "gru2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers))(gru1)\n",
    "rsh1 = tf.keras.layers.Reshape( (hidden_dim*2, 1) )(gru2)\n",
    "cov1 = tf.keras.layers.Dropout(0.3)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(rsh1)) # changed rate\n",
    "max1 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov1)\n",
    "cov2 = tf.keras.layers.Dropout(0.2)(tf.keras.layers.Conv1D(seq_len, 5, activation='relu', **regularizers)(max1)) # changed rate\n",
    "max2 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov2)\n",
    "cov3 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(max2))\n",
    "max3 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov3)\n",
    "rsh2 = tf.keras.layers.Reshape( (seq_len, 1010) )(max3)\n",
    "tdd1 = tf.keras.layers.Dropout(0.3)(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1,**regularizers))(rsh2)) # changed rate\n",
    "f1   = tf.keras.layers.Flatten()(tdd1)\n",
    "d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(f1)\n",
    "\n",
    "classifier5 = tf.keras.Model(\n",
    "    inputs=[in1], \n",
    "    outputs=[d1]\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "classifier5.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history5 = classifier5.fit(\n",
    "    np.array(x_train), np.array(y_train),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=x.shape[0] // batch_size,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-8)] # changed lr and patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history5.history['val_loss'], label='val_loss')\n",
    "plt.plot(history5.history['loss'], label='loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history5.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history5.history['accuracy'], label='accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "seq_len = x.shape[1]\n",
    "n_features = 1\n",
    "epochs = 20\n",
    "learning_rate = 0.0003\n",
    "batch_size = 16\n",
    "\n",
    "in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "gru1 = tf.keras.layers.GRU(hidden_dim, name='latent_layer')(in1)\n",
    "bn = tf.keras.layers.BatchNormalization()(gru1)\n",
    "dense = tf.keras.layers.Dense(128, activation=\"relu\")(bn)\n",
    "drop = tf.keras.layers.Dropout(0.3)(dense)\n",
    "d1   = tf.keras.layers.Dense(1, activation='sigmoid')(drop)\n",
    "\n",
    "classifier_test = tf.keras.Model(\n",
    "    inputs=[in1], \n",
    "    outputs=[d1]\n",
    ")\n",
    "\n",
    "print(classifier_test.summary())\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "classifier_test.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history_test = classifier_test.fit(\n",
    "    np.array(x_train), np.array(y_train),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=x.shape[0] // batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_test.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history_test.history['accuracy'], label='accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified RNN model - hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "seq_len = x.shape[1]\n",
    "n_features = 1\n",
    "epochs = 20\n",
    "learning_rate = 0.0003\n",
    "batch_size = 16\n",
    "\n",
    "regularizers = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "regularizers_GRU = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5),\n",
    "    'recurrent_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "gru1 = tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers_GRU)(in1)\n",
    "bn = tf.keras.layers.BatchNormalization()(gru1)\n",
    "dense = tf.keras.layers.Dense(128, activation=\"relu\", **regularizers)(bn)\n",
    "drop = tf.keras.layers.Dropout(0.3)(dense)\n",
    "d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(drop)\n",
    "\n",
    "classifier_test_opt = tf.keras.Model(\n",
    "    inputs=[in1], \n",
    "    outputs=[d1]\n",
    ")\n",
    "\n",
    "print(classifier_test_opt.summary())\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "classifier_test_opt.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history_test_opt = classifier_test_opt.fit(\n",
    "    np.array(x_train), np.array(y_train),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=x.shape[0] // batch_size,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-8)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history_test_opt.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history_test_opt.history['accuracy'], label='accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified RNN model - bigger batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "seq_len = x.shape[1]\n",
    "n_features = 1\n",
    "epochs = 20\n",
    "learning_rate = 0.0003\n",
    "batch_size = 32\n",
    "\n",
    "regularizers = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "regularizers_GRU = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5),\n",
    "    'recurrent_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "gru1 = tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers_GRU)(in1)\n",
    "bn = tf.keras.layers.BatchNormalization()(gru1)\n",
    "dense = tf.keras.layers.Dense(128, activation=\"relu\", **regularizers)(bn)\n",
    "drop = tf.keras.layers.Dropout(0.3)(dense)\n",
    "d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(drop)\n",
    "\n",
    "classifier_test_opt_b = tf.keras.Model(\n",
    "    inputs=[in1], \n",
    "    outputs=[d1]\n",
    ")\n",
    "\n",
    "print(classifier_test_opt_b.summary())\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "classifier_test_opt_b.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history_test_opt_b = classifier_test_opt_b.fit(\n",
    "    np.array(x_train), np.array(y_train),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=x.shape[0] // batch_size,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-8)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history_test_opt_b.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history_test_opt_b.history['accuracy'], label='accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified RNN model - smaller batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "seq_len = x.shape[1]\n",
    "n_features = 1\n",
    "epochs = 20\n",
    "learning_rate = 0.0003\n",
    "batch_size = 8\n",
    "\n",
    "regularizers = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "regularizers_GRU = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5),\n",
    "    'recurrent_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "gru1 = tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers_GRU)(in1)\n",
    "bn = tf.keras.layers.BatchNormalization()(gru1)\n",
    "dense = tf.keras.layers.Dense(128, activation=\"relu\", **regularizers)(bn)\n",
    "drop = tf.keras.layers.Dropout(0.3)(dense)\n",
    "d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(drop)\n",
    "\n",
    "classifier_test_opt_s = tf.keras.Model(\n",
    "    inputs=[in1], \n",
    "    outputs=[d1]\n",
    ")\n",
    "\n",
    "print(classifier_test_opt_s.summary())\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "classifier_test_opt_s.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history_test_opt_s = classifier_test_opt_s.fit(\n",
    "    np.array(x_train), np.array(y_train),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=x.shape[0] // batch_size,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-8)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history_test_opt_s.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history_test_opt_s.history['accuracy'], label='accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "seq_len = x.shape[1]\n",
    "n_features = 1\n",
    "epochs = 20\n",
    "learning_rate = 0.0003\n",
    "batch_size = 16\n",
    "\n",
    "regularizers = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "regularizers_GRU = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5),\n",
    "    'recurrent_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "gru1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers_GRU))(in1)\n",
    "bn = tf.keras.layers.BatchNormalization()(gru1)\n",
    "dense = tf.keras.layers.Dense(128, activation=\"relu\", **regularizers)(bn)\n",
    "drop = tf.keras.layers.Dropout(0.3)(dense)\n",
    "d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(drop)\n",
    "\n",
    "classifier_test_opt_bi = tf.keras.Model(\n",
    "    inputs=[in1], \n",
    "    outputs=[d1]\n",
    ")\n",
    "\n",
    "print(classifier_test_opt_bi.summary())\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "classifier_test_opt_bi.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history_test_opt_bi = classifier_test_opt_bi.fit(\n",
    "    np.array(x_train), np.array(y_train),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=x.shape[0] // batch_size,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-8)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_test_opt_bi.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history_test_opt_bi.history['accuracy'], label='accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of fall data only - distinguishing between 8 types of falls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('./UniMiB-SHAR/data/fall_data.mat')['fall_data']\n",
    "labels = scipy.io.loadmat('./UniMiB-SHAR/data/fall_labels.mat')['fall_labels']\n",
    "classes = np.unique(labels[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLASSES = len(classes)\n",
    "\n",
    "batch_size = 1\n",
    "bs = batch_size * NUMBER_OF_CLASSES # for ballancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idxs = {cl: np.where(labels[:, 0] == cl)[0] for cl in classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of samples in each class:\")\n",
    "[len(class_idxs[i]) for i in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.reshape(data.shape[0], data.shape[1], 1)\n",
    "y = labels[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_encoding = {\n",
    "    1: [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    2: [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    3: [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    4: [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    5: [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    6: [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    7: [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    8: [0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    \n",
    "}\n",
    "\n",
    "y_encoded = [label_to_encoding[yy] for yy in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "idx_shuffle = np.arange(len(x))\n",
    "np.random.shuffle(idx_shuffle)\n",
    "train_idxs = idx_shuffle[:int(len(x)*0.8)]\n",
    "val_idxs = idx_shuffle[int(len(x)*0.8):]\n",
    "x_train = np.array(x)[train_idxs]\n",
    "x_val = np.array(x)[val_idxs]\n",
    "y_train_encoded = np.array(y_encoded)[train_idxs]\n",
    "y_val_encoded = np.array(y_encoded)[val_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indexes of each group\n",
    "cl_indexes = {cl:np.where(y==cl)[0] for cl in classes}\n",
    "\n",
    "# take the the samples to test set (equal number for each class)\n",
    "number_of_test_samples = int(len(x)*0.2)\n",
    "number_of_test_samples_per_class = number_of_test_samples // NUMBER_OF_CLASSES\n",
    "\n",
    "val_idxs = [item for cl, items in cl_indexes.items() for item in items[:number_of_test_samples_per_class]]\n",
    "train_idxs = [item for cl, items in cl_indexes.items() for item in items[number_of_test_samples_per_class:]]\n",
    "\n",
    "x_val = x[val_idxs]\n",
    "y_val = y[val_idxs]\n",
    "y_val_encoded = np.array(y_encoded)[val_idxs]\n",
    "\n",
    "x_train = x[train_idxs]\n",
    "y_train = y[train_idxs]\n",
    "y_train_encoded = np.array(y_encoded)[train_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idxs = {cl: np.where(y_train == cl)[0] for cl in classes}\n",
    "print(\"Number of samples in each class:\")\n",
    "[len(class_idxs[i]) for i in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idxs = {cl: np.where(y_val == cl)[0] for cl in classes}\n",
    "print(\"Number of samples in each class:\")\n",
    "[len(class_idxs[i]) for i in classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on optimized CNN-infused model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "seq_len = x.shape[1]\n",
    "n_features = 1\n",
    "epochs = 60\n",
    "learning_rate = 0.0003\n",
    "batch_size = 16\n",
    "\n",
    "regularizers = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "gru1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, return_sequences=True, **regularizers))(in1)\n",
    "gru2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers))(gru1)\n",
    "rsh1 = tf.keras.layers.Reshape( (hidden_dim*2, 1) )(gru2)\n",
    "cov1 = tf.keras.layers.Dropout(0.3)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(rsh1)) # changed rate\n",
    "max1 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov1)\n",
    "cov2 = tf.keras.layers.Dropout(0.2)(tf.keras.layers.Conv1D(seq_len, 5, activation='relu', **regularizers)(max1)) # changed rate\n",
    "max2 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov2)\n",
    "cov3 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(max2))\n",
    "max3 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov3)\n",
    "rsh2 = tf.keras.layers.Reshape( (seq_len, 1010) )(max3)\n",
    "tdd1 = tf.keras.layers.Dropout(0.3)(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1,**regularizers))(rsh2)) # changed rate\n",
    "f1   = tf.keras.layers.Flatten()(tdd1)\n",
    "d1   = tf.keras.layers.Dense(8, activation='softmax', **regularizers)(f1)\n",
    "\n",
    "classifier_f = tf.keras.Model(\n",
    "    inputs=[in1], \n",
    "    outputs=[d1]\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "classifier_f.compile(loss='categorical_crossentropy', optimizer=opt, \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history_f = classifier_f.fit(\n",
    "    x_train, y_train_encoded, batch_size=batch_size, epochs=epochs,\n",
    "    validation_data=(x_val, y_val_encoded),\n",
    "    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-8)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_f.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history_f.history['accuracy'], label='accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on final Bidirectional RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "seq_len = x.shape[1]\n",
    "n_features = 1\n",
    "epochs = 60\n",
    "learning_rate = 0.0003\n",
    "batch_size = 16\n",
    "\n",
    "regularizers = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "regularizers_GRU = {\n",
    "    'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "    'activity_regularizer':tf.keras.regularizers.l2(1e-5),\n",
    "    'recurrent_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "}\n",
    "\n",
    "in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "gru1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers_GRU))(in1)\n",
    "bn = tf.keras.layers.BatchNormalization()(gru1)\n",
    "dense = tf.keras.layers.Dense(128, activation=\"relu\", **regularizers)(bn)\n",
    "drop = tf.keras.layers.Dropout(0.3)(dense)\n",
    "d1   = tf.keras.layers.Dense(8, activation='softmax', **regularizers)(drop)\n",
    "\n",
    "classifier_f_bi = tf.keras.Model(\n",
    "    inputs=[in1], \n",
    "    outputs=[d1]\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "classifier_f_bi.compile(loss='categorical_crossentropy', optimizer=opt, \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history_f_bi = classifier_f_bi.fit(\n",
    "    x_train, y_train_encoded, batch_size=batch_size, epochs=epochs,\n",
    "    validation_data=(x_val, y_val_encoded),\n",
    "    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-8)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_f_bi.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history_f_bi.history['accuracy'], label='accuracy')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2.8",
   "language": "python",
   "name": "tf_2.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
